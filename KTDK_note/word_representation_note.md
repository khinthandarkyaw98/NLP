# Warning, this is just my scratch note, not well-prepared document !!!!

## I just aim to use this note for my personal review later on. It may not be helpful to you cos it is not well-planned and organized one.

> Let's go from one-hot representation to featurized representation!!! This is also known as the word embedding.
For more details, [check this out](https://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation).

> Smaller datasets with word embedding can be used together with Transfer Learning( Learned from the larger corpus that will be appiled to the smaller dataset ). 
For more details, [check this out](https://www.coursera.org/learn/nlp-sequence-models/lecture/qHMK5/using-word-embeddings).
